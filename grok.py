import os
import streamlit as st 
from dotenv import load_dotenv 

# --- IMPORTACIONES DE LLAMA_INDEX ---
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    Settings,
    PromptTemplate,
    StorageContext,
    load_index_from_storage
) 

from llama_index.core.node_parser import SentenceSplitter
# --- CAMBIOS EN IMPORTACIONES ---
from llama_index.embeddings.gemini import GeminiEmbedding # (Para Embeddings)
from llama_index.llms.groq import Groq                   # (Para el LLM)
# ------------------------------------

# Cargar variables de .env (solo para desarrollo local)
load_dotenv()

# --- CONFIGURACI칍N INICIAL ---
RUTA_DE_TUS_DOCUMENTOS = "./PDF" 
RUTA_PERSISTENCIA = "./storage_vectorial" 
# ----------------------------------------


# --- INTERFAZ PRINCIPAL DE STREAMLIT ---
st.title("游뱄 Chat con tus Documentos (con Groq 游 + Gemini 游눑)")
st.caption("Esta app usa Groq para el LLM y Google Gemini para los Embeddings.")

# --- BARRA LATERAL (SIDEBAR) PARA CONFIGURACI칍N ---
with st.sidebar:
    
    # Logo
    try:
        st.image("logo.png", width=150) 
    except FileNotFoundError:
        st.warning("No se encontr칩 'logo.png'.")

    st.title("Configuraci칩n")
    st.markdown("Ajusta los par치metros antes de iniciar el chat.")

    # Bot칩n de Refresco
    if st.button("游댃 Refrescar Listas (Archivos/Modelos)"):
        st.cache_data.clear()
        st.rerun()

    # --- 1. L칍GICA DE SELECCI칍N DE ARCHIVOS ---
    st.divider()
    st.subheader("1. Selecci칩n de Archivo")
    
    @st.cache_data
    def escanear_archivos(ruta_base):
        archivos_unicos = set()
        try:
            for root, _, files in os.walk(ruta_base):
                for file in files:
                    if file.lower().endswith(".pdf"):
                        archivos_unicos.add(os.path.join(root, file))
        except Exception as e:
            st.error(f"Error al escanear la carpeta: {e}")
            return []
        
        lista_archivos = [
            (os.path.basename(ruta), ruta) for ruta in sorted(list(archivos_unicos))
        ]
        return lista_archivos

    with st.spinner("Escaneando documentos..."):
        lista_archivos_tuplas = escanear_archivos(RUTA_DE_TUS_DOCUMENTOS)
        
    opciones_archivos = {
        "Todos los archivos": RUTA_DE_TUS_DOCUMENTOS,
        **{nombre: ruta for nombre, ruta in lista_archivos_tuplas}
    }

    archivo_seleccionado_nombre = st.selectbox(
        "Elige un archivo (o todos):",
        options=opciones_archivos.keys()
    )
    ruta_a_cargar = opciones_archivos[archivo_seleccionado_nombre]

    # --- 2. L칍GICA DE SELECCI칍N DE LLM (Groq) ---
    st.divider()
    st.subheader("2. Selecci칩n de LLM (Groq)")

    # --- INICIO DE LA MODIFICACI칍N (Lista de Modelos Groq) ---
    # Modelos actualizados recomendados por Groq
    modelos_disponibles = [
        "llama-3.1-8b-instant",  # El m치s r치pido y nuevo (reemplaza a llama3-8b)
        "llama-3.1-70b-versatile", # El m치s potente (reemplaza a llama3-70b)
        "gemma2-9b-it"           # El nuevo Gemma (reemplaza a gemma-7b)
    ]
    # --- FIN DE LA MODIFICACI칍N ---

    LLM_MODEL = st.selectbox(
        "Elige un modelo LLM de Groq:",
        options=modelos_disponibles,
        index=0 
    )

    # --- 3. CONFIGURACI칍N DE CHUNKS ---
    st.divider()
    st.subheader("3. Configuraci칩n de Chunks")
    chunk_size = st.number_input("Tama침o del chunk (Chunk Size):", min_value=100, max_value=8000, value=1000, step=100)
    chunk_overlap = st.number_input("Solapamiento (Chunk Overlap):", min_value=0, max_value=1000, value=200, step=50)


# --- L칍GICA PRINCIPAL (CARGA Y CREACI칍N DEL 칈NDICE) ---
@st.cache_resource(show_spinner="Cargando o creando el 칤ndice...")
def cargar_y_crear_indice(_ruta_a_cargar, _llm_model, _chunk_size, _chunk_overlap):
    
    # 1. Crear un nombre de directorio 칰nico
    # Limpiamos el nombre base para que sea un nombre de carpeta v치lido
    if os.path.isfile(_ruta_a_cargar):
        nombre_base_limpio = os.path.basename(_ruta_a_cargar).replace('.', '_').replace(' ', '')
    else:
        # Usamos un nombre gen칠rico si se seleccionan "Todos los archivos"
        nombre_base_limpio = "Todos_los_archivos"

    # Incluimos el modelo de embedding en el nombre para evitar conflictos
    directorio_persistencia = os.path.join(
        RUTA_PERSISTENCIA, 
        f"gemini_embeddings_{nombre_base_limpio}_chunk{_chunk_size}_overlap{_chunk_overlap}"
    )
    
    # 2. Configurar el LLM (Groq) y Embeddings (Gemini)
    
    # Leemos las API keys (funcionar치 local con .env y en la nube con Secrets)
    groq_api_key = os.environ.get("GROQ_API_KEY")
    google_api_key = os.environ.get("GOOGLE_API_KEY") 

    # Comprobamos las claves
    if not groq_api_key:
        st.error("No se encontr칩 GROQ_API_KEY. A침치dela a .env o a los Secrets de Streamlit.")
        return None
    if not google_api_key: 
        st.error("No se encontr칩 GOOGLE_API_KEY. A침치dela a .env o a los Secrets de Streamlit.")
        return None
        
    st.write(f"Conectando a Groq (Modelo: {_llm_model})...")
    Settings.llm = Groq(
        model=_llm_model, 
        api_key=groq_api_key
    )
    
    # Usamos Gemini para los Embeddings
    st.write(f"Cargando Embedding Model: Gemini (models/embedding-001)")
    Settings.embed_model = GeminiEmbedding(
        model_name="models/embedding-001",
        api_key=google_api_key 
    )

    Settings.text_splitter = SentenceSplitter(
        chunk_size=_chunk_size, 
        chunk_overlap=_chunk_overlap
    )

    # 3. Comprobar si el 칤ndice ya existe en disco
    if not os.path.exists(directorio_persistencia):
        st.info("칈ndice no encontrado. Creando y guardando uno nuevo... (Esto puede tardar)")
        
        if os.path.isfile(_ruta_a_cargar):
            loader = SimpleDirectoryReader(input_files=[_ruta_a_cargar])
        else:
            loader = SimpleDirectoryReader(input_dir=_ruta_a_cargar, recursive=True)
            
        try:
            documents = loader.load_data(show_progress=True)
            if not documents:
                st.error("No se pudieron cargar documentos. Revisa la ruta.")
                return None
        except Exception as e:
            st.error(f"Error cr칤tico al cargar documentos: {e}")
            return None

        index = VectorStoreIndex.from_documents(
            documents,
            show_progress=True
        )
        
        st.write(f"Guardando 칤ndice en: {directorio_persistencia}")
        index.storage_context.persist(persist_dir=directorio_persistencia)
        st.success("춰칈ndICE creado y guardado!")

    else:
        st.info(f"Cargando 칤ndice existente desde: {directorio_persistencia}")
        storage_context = StorageContext.from_defaults(persist_dir=directorio_persistencia)
        index = load_index_from_storage(storage_context)
        st.success("춰칈ndice cargado desde disco!")
    
    # 4. Definir el Prompt Template
    prompt_template_str = """
    Eres un asistente de IA que SOLO habla espa침ol. Tu 칰nica tarea es responder en espa침ol.
    Responde la pregunta del usuario bas치ndote estricta y 칰nicamente en el contexto proporcionado.
    Si la respuesta no se encuentra en el contexto, debes decir: "No tengo informaci칩n sobre eso".
    Bajo NINGUNA circunstancia respondas en ingl칠s.

    Contexto:
    <context>
    {context_str}
    </context>

    Pregunta: {query_str}

    Respuesta (en espa침ol):
    """
    qa_template = PromptTemplate(prompt_template_str)

    # 5. Crear el Motor de Consulta y devolverlo
    query_engine = index.as_query_engine(
        text_qa_template=qa_template,
        similarity_top_k=3 
    )
    return query_engine

# --- FIN DE LA FUNCI칍N DE CACH칄 ---


# Inicializamos query_engine como None ANTES del try
query_engine = None 

try:
    query_engine = cargar_y_crear_indice(
        ruta_a_cargar,
        LLM_MODEL,
        chunk_size,
        chunk_overlap
    )
except Exception as e:
    st.error(f"Error al inicializar el motor de consulta: {e}")
    
# Esta comprobaci칩n ahora funciona de forma segura
if query_engine is None:
    st.error("No se pudo inicializar el motor de consulta. Revisa la configuraci칩n y los documentos.")
    st.stop()


# --- L칍GICA DE LA INTERFAZ DE CHAT ---

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "춰Hola! Estoy listo para responder preguntas sobre tus documentos."}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Escribe tu pregunta aqu칤..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty() 
        
        with st.spinner("Pensando... (Usando Groq 游 + Gemini 游눑)"):
            try:
                response = query_engine.query(prompt)
                full_response = str(response)
                
            except Exception as e:
                full_response = f"Error al procesar la consulta: {e}"
        
        message_placeholder.markdown(full_response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})